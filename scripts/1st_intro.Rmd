---
title: "1st_intro"
author: "Lim Yong-Hyun"
date: '2022 7 6 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```
<목차>

# R을 이용한 웹 크롤링?

## 크롤링이란?
### sample code 분석
### sample code를 모방한 자료찾기 코드 만들기
### 

## 방법들
### R vs. Python?
### R을 이용한 크롤링의 장점은?

## 응용

```

# R을 이용한 웹 크롤링?

## 크롤링이란?

### sample code를 분석해 보기

3개의 패키지를 이용한다. `rvest`, `httr`가 웹 스크래핑(관련한 깃허브에는 '웹스크래핑'으로 표현)에 관여하는 패키지이며, `tidyverse`는 코드 전처리 등에 사용하는 패키지이다. `rvest`의 비네트를 찾아보니, 깃허브 레포와 연결돼 있다. 다음 URL 참조할 것. (https://github.com/yusuzech/r-web-scraping-cheat-sheet). 아래의 코드는 오래 전에 긁어온 것인데, 어디서 가져왔는지 알 수 없다. 

```{r}

library(rvest)
library(httr)
library(tidyverse)

```
* 라이브러리 import

### 이하 샘플코드 및 연습습


```{r}

# Settings
years = 2020:2022

searches = list(
  smart_home = '"smart home" OR "IoT based care" OR "smart care" OR "smart ADL coaching" OR "IoT support daily care"',
  care_group = '"dementia" OR "AD" OR "MCI" OR "Normal control"'
)

sleep_interval = c(1, 10)  # Uniformly break between searches in this interval to prevent scholar from rejecting searches

scholar_prefix = 'https://scholar.google.dk/scholar?hl=en&as_sdt=0%2C5&as_ylo=9999&as_yhi=9999&q='

```
* years: 검색할 년도 입력
* searches: 검색할 키워드 입력, 리스트 형식
* sleep_interval: 로봇으로 인식할 수 없도록 검색 시간 간격 설정
* scholar_prefix: 검색할 사이트 주소


함수를 4 개 만든다. 
```{r}
###################
# HANDY FUNCTIONS #
###################

# Build the URL string
get_url = function(keyword, year) {
  url_prefix = gsub('9999', as.character(year), scholar_prefix)  # Enter year
  url_search = gsub(' ', '+', searches[[keyword]])  # Escape spaces
  url_search = gsub('\"', '%22', url_search)  # Escape quotes
  url = paste(url_prefix, url_search, sep='')
  url
}

```
* 




```{r}

# Do the web search
get_html = function(url) {
  html = read_html(url)
  #html = content(GET(url))
  html
}

```

```{r}

extract_citations = function(html) {
  # Extract the citation number
  hits_strings = html %>%
    html_nodes(css='.gs_ab_mdw') %>%  # Name of the class where we can find citation number
    html_text()
  hits_string = strsplit(hits_strings[2], ' ')[[1]][2]  # Second hit, second "word"
  # hits_numeric = as.numeric(gsub(',', '', hits_string))  # As numeric, not string
  # hits_numeric
  hits_string
}
```

```{r}
get_citations = function(keyword, year) {
  # Sleep to prevent HTTP error 503
  sleep_duration = runif(1, sleep_interval[1], sleep_interval[2])
  Sys.sleep(sleep_duration)
  
  # Do the search
  url = get_url(keyword, year)
  html = get_html(url)
  citations =  extract_citations(html)
  
  # Status and return
  print(sprintf('Got %i scholar citations in %i for %s', citations, year, keyword))
  citations
}
```






```{r}
#################
# DO THE SEARCH #
#################
citation_history = expand.grid(years, names(searches))
names(citation_history) = c('year', 'keyword')

citation_history = citation_history %>%
  rowwise() %>%
  mutate(
    citations = get_citations(keyword, year)
  )

# Save it so you don't have to repeat in case Scholar locks you out
write.csv(citation_history, 'data/citations.csv', row.names = F)

```

### 다른 샘플

```{r}
url_tvcast <- "http://tvcast.naver.com/jtbc.youth"
html_tvcast <- read_html(url_tvcast, encoding = "UTF-8")

html_tvcast %>% html_nodes(".title a")
html_tvcast %>% html_nodes(".title a") %>% html_text()
tvcast_df <- html_tvcast %>% html_nodes(".title a") %>% html_text() %>% data.frame()

```



```{r}
url_tvcast <- "https://www.kobis.or.kr/kobis/business/stat/boxs/findDailyBoxOfficeList.do"
html_tvcast <- read_html(url_tvcast)

html_tvcast %>% html_nodes(css = '.ellip per90')
html_tvcast %>% html_nodes(css = '.ellip per90') %>% html_text()
tvcast_df <- html_tvcast %>% html_nodes(css = '.ellip per90') %>% html_text() %>% data.frame()


```
https://tv.naver.com/jtbc.youth


https://cinema4dr12.tistory.com/1170